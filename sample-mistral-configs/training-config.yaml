---
inherit:
    - gpt2-small.yaml

training_arguments:
    # Set these based on GPU RAM/your available hardware
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 4

    # Learning Rate & Optimization Parameters, assumes AdamW
    adam_beta2: 0.95

    # Maximum Training Steps (Overrides epochs!)
    max_steps: 10000

    # LR Scheduling Parameters -- Warmup Steps should be 1% of total steps (Could use ratio)
    warmup_steps: 1000

    # Saving and Evaluation Steps
    eval_steps: 1000
    save_steps: 1000
